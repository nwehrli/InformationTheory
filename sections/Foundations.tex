\section{Foundations}
\begin{mainbox}{Definitions}
    \(\text{Information of an outcome }x\)
    \[h(x) = - \log(p(x))\]
    \(\text{Cross-Entropy between }p \text{ and }q\)
    \[H(p; q) = - \sum_{x}p(x) \log q(x)\]
    \text{Shannon Entropy}
    \[H(p) = H(p; p) \]
\end{mainbox}    
\textbf{Notation} 

We identify outcomes \(x\) with integers \(1, ..., m\) and associate probabilities \(p(x) \geq 0\).

\(H(\frac{1}{m})\) for \(H(p)\) with \(p(x) = \frac{1}{m}\) (uniform)

\(H(X) = H(p) = \E(-\log(p(X)))\) where \(p\) is the pdf of \(X\)

% \begin{mainbox}
%     {Convexity and Concavity}
%     \begin{itemize}
%         \item  A function \(f: \mathcal{D} \to \R\) over some convex Domain \(\mathcal{D}\) is\\ \textbf{(strictly) convex}, 
%         if \(\forall z \neq z' \in \mathcal{D}, \forall \rho \in (0,1)\)
%         \[f(\rho z + (1-\rho)z') \leq \rho f(z) + (1-\rho)f(z')\]
%         \item \(g\) is \textbf{(strictly) concave} \(\iff\) \(f = - g \) is (strictly) convex
%         \item \(f''(z) \geq 0, \forall z \in \mathcal{D} \iff f \text{ is convex over }\mathcal{D}\\
%                 f''(z) > 0, \forall z \in \mathcal{D} \implies f \text{ is strictly convex over }\mathcal{D}\)
%         \item \(f(\rho) = -\rho \log \rho \text{ is strictly concave for }\rho > 0\\
%             H(p) \text{ is strictly concave}\)
%     \end{itemize}
% \end{mainbox}
\vspace*{1mm}
\textbf{Jensen's Inequality}

    Let \(f\) be convex and \(g: [m] \to \R\) be an arbitrary function that assigns a value to each outcome.
    \[f\left(\sum_{x}p(x)g(x)\right) \leq \sum_{x} p(x) f(g(x)), \forall p(x) \geq 0, \sum_x p(x) = 1\]
    alternatively
    \[f(\E(g(X))) \leq \E(f(g(X)))\]


Applying this inequality to relate \textbf{Cross-Entropy} and \textbf{Entropy}, we get the following properties.
\[H(p; q) \geq H(p)\]
    Definining \textbf{KL divergence} or \textbf{Relative Entropy} as
    \[D(p || q) = \sum_{x} p(x) \log \frac{p(x)}{q(x)}\]
    we get
    \[H(p; q) = H(p) + D(p || q)\]
    Further investigating KL divergence, we find 
    \begin{align}
        D(p||q) &\geq 0\\
        D(p||q) = 0 &\iff p = q
    \end{align}
    A further consequence of (1) is that the uniform distribution \textbf{maximizes} entropy.
    \begin{align*}
        H\left(\frac{1}{m}\right) = \max_{p}H(p)
    \end{align*}
    \begin{mainbox}
        {Definitions - Conditional distributions}
        Conditional information
        \[h(x|y) = - \log p(x|y)\]
        Conditional Entropy
        \begin{align*}
            H(X | Y = y) &= - \sum_{x}p(x|y) \log p(x|y)\\
            H(X|Y) &= \sum_{y} p(y)H(X|Y = y)
        \end{align*}
        Monotonicity of Conditioning
        \[H(X|Y) \leq H(X)\]
    \end{mainbox}
    \textbf{Joint Entropy}

    \[H(X, Y) = - \sum_{x, y}p(x,y) \log p(x, y)\]

    \textbf{Chain Rule}

    \[H(X, Y) = H(X | Y) + H(Y)\]

    \textbf{Subadditivity}

    \[H(X, Y) \leq H(X) + H(Y)\]
    with equality if \(X \perp Y\).

    \textbf{Multiple Conditioning}

    \[H(X | Y, Y') \leq H(X|Y)\]

    Generalized to \(X_1, ..., X_n\) we get
    \[H(X_1, ..., X_n) = \sum_{i = 1}^n H(X_i | X_1, ..., X_{i-1}) \leq \sum_{i = 1}^n H(X_i)\]

    \textbf{Mutual Information}
    \[I(X; Y) := H(X) - H(X|Y) = H(Y) - H(Y|X)\]
    We further have 
    \[I(X; Y) = D(P(X, Y) || P(X)P(Y))\]
    with \(I(X;Y) = 0\) if \(X \perp Y\).

    \textbf{Conditional Mutual Information}
    \[I(X; Y|Z) := H(X|Z) - H(X|Y, Z)\]

    \textbf{Conditional Independence}
    
    If \(X \perp Y|Z\) 
    \[I(X; Y | Z) = 0 \text{ and }I(X;Y) \leq I(X;Z)\]

    We can deduct that for any function \(\phi\) on outcomes of \(X\)
    \[I(\phi(X); Y) \leq I(X;Y)\]




